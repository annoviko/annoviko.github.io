---
layout: post
title: Clustering using G-Means algorithm
---

Hi everyone! In this post, I would like to describe the G-Means clustering algorithm. Why G-Means? Because this algorithm may work without any prior information, there is no need to specify an amount of clusters that should be extracted from a dataset or initial centers or any other parameters. Of course, it has limitations and it is not a panacea for data mining area.

What is the G-Means algorithm? G-Means algorithm was proposed by Greg Hamerly and Charles Elkan in their paper "Learning the k in k-means". The idea of the algorithm is discover an appropriate amount of cluster using a statistical test to decide whether to split a K-Means center into two centers. Sounds like X-Means algorithm where the algorithm uses the Bayesian Information Criterion (BIC) instead of the statistical test. But, the BIC is ineffective as a scoring function, since it does not penalize strongly enough the model’s complexity.

The G-Means algorithm starts search from K (it can be equal to 1) number of clusters. Each iteration, the algorithm splits each cluster (that were obtained on the previous iteration) into two centers if their data appear not to come from a Gaussian distribution. The Anderson-Darling statistical test for a Gaussian distribution is used to make this splitting decision.

Let’s check out how the algorithm works on practice. I am going to use pyclustering library for that purpose. pyclustering 0.9.2 contains Python and C++ implementations of this algorithm. `gmeans` receives three arguments: `data`, `k_init` and `repeat`. `data` is just a list of points that should be clustered, `k_init` is an initial number of centers, `repeat` is a value that defines how many times K-Means with K-Means++ should be run to find out optimal clusters. Suppose we do not know anything about input samples, so let’s set `k_init` to 5. In addition, set `repeat` to 5 to get more or less optimal clusters (you can play with that parameter to see how it affects clustering results).

Here is an example of clustering that is written in Python:

```python
from pyclustering.cluster import cluster_visualizer
from pyclustering.cluster.gmeans import gmeans
from pyclustering.utils import read_sample
from pyclustering.samples.definitions import FCPS_SAMPLES, SIMPLE_SAMPLES

def gmeans_clustering(path_sample):
    # Read sample from file.
    sample = read_sample(path_sample)

    # Create instance of G-Means algorithm. 
	# By default algorithm starts search from a single cluster.
    gmeans_instance = gmeans(sample, repeat=5).process()

    # Extract clustering results: clusters and their centers
    clusters = gmeans_instance.get_clusters()

    # Visualize clustering results
    visualizer = cluster_visualizer()
    visualizer.append_clusters(clusters, sample)
    visualizer.show()


gmeans_clustering(SIMPLE_SAMPLES.SAMPLE_SIMPLE1)
gmeans_clustering(SIMPLE_SAMPLES.SAMPLE_SIMPLE2)
gmeans_clustering(SIMPLE_SAMPLES.SAMPLE_SIMPLE3)
# ... other samples
gmeans_clustering(FCPS_SAMPLES.SAMPLE_TETRA)
```

Clustering results for `Simple` and `FCPS` data sets are presented below.
<p align="center">
  <img src="images/post/21-11-2019/g-means_simple_clustering.png">
</p>
<p align="center">
	<b>Fig. 1.</b> 'Simple' samples processed by G-Means algorithm.
</p>

<p align="center">
  <img src="images/post/21-11-2019/g-means_fcps_clustering.png">
</p>
<p align="center">
	<b>Fig. 2.</b> 'FCPS' samples processed by G-Means algorithm.
</p>

Indeed, G-Means is able to extract proper well-scattered clusters with Gaussian distribution. Of course, the algorithm inherits all disadvantages from K-Means and K-Means++, but it is able to find optimal (almost optimal – quasi-optimal) clusters taking into account mentioned restrictions. Just try it on practice by yourself.

I mentioned that pyclustering contains C++ implementation as well. Does it mean that the library uses C++ implementation to increase performance? Yes, that is correct, but also it means that C++ implementation can be used in your C++ project. Here is a C++ example of clustering by the same G-Means algorithm.

```cpp
#include <pyclustering/cluster/gmeans.hpp>

#include <fstream>
#include <iostream>
#include <sstream>

using namespace pyclustering;
using namespace pyclustering::clst;

dataset read_data(const std::string & filename) {
/* Implementation of reading input data. */
}

int main() {
    /* Data represented as { { 1.0, 1.2 }, { 1.3, 1.2 }, { 1.1, 1.3 }, ... */
    dataset data = read_data("simple1.txt");
    
    gmeans_data result;
    gmeans(1).process(data, result);

    for (auto group : result.clusters()) {
        std::cout << "[ ";
        for (auto index : group) { std::cout << index << " "; }
        std::cout << "]";
    }

    return 0;
}
```

Here is an example how input data can read from simple text file.

```cpp
dataset read_data(const std::string & filename) {
    dataset data;

    std::ifstream file(filename);
    std::string line;

    while (std::getline(file, line)) {
        std::stringstream stream(line);

        point coordinates;
        double value;
        while (stream >> value) {
            coordinates.push_back(value);
        }

        data.push_back(coordinates);
    }

    file.clear();
    return data;
}
```

The input data is a container of points, there is an example below:

```
dataset data = { { 1.3, 3.1 }, { 2.1, 2.4 }, { 1.1, 1.9 }, { 4.5, 6.3 } };
```

Just make sure that your compiler supports C++14 and if it does then enjoy G-Means in C++.

If you want to learn more about the G-Means algorithm, there is the article reference:

Greg Hamerly and Charles Elkan. Learning the k in k-means. In _Proceedings of the 16th International Conference on Neural Information Processing Systems_, NIPS'03, pages 281–288, Cambridge, MA, USA, 2003. MIT Press.
